{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PROJECT TITLE : Enchanted Wings: Marvels Of Butterfly Species\n",
        "\n",
        "Team ID : LTVIP2025TMID40802\n",
        "\n",
        "Team Size : 4\n",
        "\n",
        "Team member : Mannepalli Abhilash\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GYzeus2xJW7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Enchanted Wings\" project aims to develop a robust butterfly image classification model using transfer learning, addressing key applications in biodiversity monitoring, ecological research, and citizen science. By leveraging a dataset of 75 butterfly species and 6499 images, the project focuses on efficient and accurate species identification through pre-trained Convolutional Neural Networks (CNNs).\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This project will detail the end-to-end process of building and deploying a butterfly image classification system, encompassing:\n",
        "\n",
        "  * **Prerequisites:** Essential knowledge, software, and hardware.\n",
        "  * **Architecture:** The overall deep learning pipeline and system design.\n",
        "  * **Project Structure:** A well-organized directory for efficient development.\n",
        "  * **Data Collection and Preparation:** Strategies for acquiring, cleaning, and augmenting image data.\n",
        "  * **Dataset Splitting:** Methodologies for partitioning data into training, validation, and test sets.\n",
        "  * **Model Building:** Implementing transfer learning with pre-trained CNNs.\n",
        "  * **Model Testing and Performance Evaluation:** Assessing model efficacy using various metrics.\n",
        "  * **Data Production Application Building:** Deploying the model for real-world use.\n",
        "\n",
        "## 1\\. Prerequisites\n",
        "\n",
        "To successfully execute this project, the following prerequisites are recommended:\n",
        "\n",
        "  * **1.1. Foundational Knowledge:**\n",
        "\n",
        "      * **Python Programming:** Proficiency in Python, including object-oriented programming concepts.\n",
        "      * **Machine Learning Basics:** Understanding of supervised learning, classification, and model evaluation.\n",
        "      * **Deep Learning Concepts:** Familiarity with neural networks, backpropagation, convolutional layers, and activation functions.\n",
        "      * **Linear Algebra & Calculus:** Basic understanding of matrix operations, gradients, and optimization.\n",
        "      * **Image Processing Fundamentals:** Concepts like pixel manipulation, image channels, and transformations.\n",
        "\n",
        "  * **1.2. Software Requirements:**\n",
        "\n",
        "      * **Python 3.8+:** The primary programming language.\n",
        "      * **Deep Learning Frameworks:**\n",
        "          * `TensorFlow 2.x` or `PyTorch`: For building and training neural networks. (TensorFlow/Keras will be used in examples).\n",
        "      * **Essential Libraries:**\n",
        "          * `NumPy`: For numerical operations.\n",
        "          * `Pandas`: For data manipulation (if metadata is involved).\n",
        "          * `Matplotlib`, `Seaborn`: For data visualization.\n",
        "          * `scikit-learn`: For utility functions like data splitting and metrics.\n",
        "          * `Pillow (PIL)` or `OpenCV`: For image loading and manipulation.\n",
        "          * `Flask` or `FastAPI`: For building the web API.\n",
        "          * `Docker`: For containerization.\n",
        "      * **Integrated Development Environment (IDE):**\n",
        "          * `Jupyter Notebook` / `JupyterLab`: For experimentation and rapid prototyping.\n",
        "          * `VS Code` / `PyCharm`: For structured code development.\n",
        "      * **Version Control:** `Git` and a platform like GitHub/GitLab for collaborative development and code management.\n",
        "\n",
        "  * **1.3. Hardware Requirements:**\n",
        "\n",
        "      * **CPU:** A modern multi-core processor for general computations.\n",
        "      * **GPU (Recommended):** A powerful NVIDIA GPU (e.g., RTX 30-series, A100) with sufficient VRAM (at least 8GB, preferably 16GB+) is highly recommended for accelerated model training, especially for large datasets and complex CNNs.\n",
        "      * **RAM:** At least 16GB, preferably 32GB or more, for handling image datasets.\n",
        "      * **Storage:** Ample SSD storage (256GB+) for datasets, models, and development environments.\n",
        "\n",
        "  * **1.4. Development Environment Setup:**\n",
        "\n",
        "      * **Virtual Environments:** Use `venv` or `Conda` to manage project dependencies isolation.\n",
        "      * **Cloud Platforms (Optional but Recommended for larger scale):** Google Colab, Kaggle Kernels, AWS SageMaker, Google Cloud AI Platform, Azure Machine Learning offer managed environments with GPU access.\n",
        "\n",
        "## 2\\. Architecture\n",
        "\n",
        "The project's architecture can be broken down into the deep learning pipeline and the overall system architecture for deployment.\n",
        "\n",
        "### 2.1. Deep Learning Pipeline Architecture\n",
        "\n",
        "The core deep learning pipeline involves several stages:\n",
        "\n",
        "1.  **Data Ingestion:**\n",
        "      * **Input:** Raw butterfly images (e.g., JPEG, PNG) and associated metadata (species labels).\n",
        "      * **Process:** Loading images from disk, potentially from structured directories or a database.\n",
        "2.  **Data Preprocessing & Augmentation:**\n",
        "      * **Process:** Resizing images to a uniform dimension suitable for the CNN, normalizing pixel values, and applying data augmentation techniques (e.g., rotation, flipping, zooming, color jitter, CutMix, Mixup, RandAugment) to increase dataset diversity and prevent overfitting.\n",
        "      * **Output:** Batches of preprocessed and augmented image tensors.\n",
        "3.  **Dataset Splitting:**\n",
        "      * **Process:** Partitioning the data into training, validation, and test sets. Crucially, stratified sampling is used to ensure each subset has a representative distribution of butterfly species, especially vital for 75 classes. Techniques for handling class imbalance (e.g., oversampling, weighted loss) are applied here.\n",
        "      * **Output:** Three distinct datasets: training, validation, and testing.\n",
        "4.  **Model Building (Transfer Learning):**\n",
        "      * **Process:** Loading a pre-trained CNN (e.g., ResNet50, MobileNetV2, EfficientNet) which has learned robust features from a large dataset like ImageNet. The top classification layers are removed, and new custom layers are added (e.g., Global Average Pooling, Dense layers, Dropout) suitable for 75 butterfly classes.\n",
        "          * **Feature Extraction:** Initially freezing the base pre-trained layers and training only the new classification head.\n",
        "          * **Fine-tuning:** Unfreezing some or all of the base layers and retraining them with a very low learning rate alongside the new head.\n",
        "      * **Output:** A configured deep learning model ready for training.\n",
        "5.  **Model Training:**\n",
        "      * **Process:** Iteratively feeding training data to the model, computing loss, and updating model weights using an optimizer (e.g., Adam, SGD). Validation data is used to monitor performance and prevent overfitting.\n",
        "      * **Output:** A trained model (model weights and architecture).\n",
        "6.  **Model Evaluation:**\n",
        "      * **Process:** Assessing the trained model's performance on the unseen test set using various metrics (Accuracy, Precision, Recall, F1-score, Confusion Matrix).\n",
        "      * **Output:** Performance metrics and insights into model strengths/weaknesses.\n",
        "7.  **Model Deployment:**\n",
        "      * **Process:** Exporting the trained model into a production-ready format (e.g., SavedModel for TensorFlow, TorchScript for PyTorch). Packaging the model and its inference logic within a container (Docker). Exposing a RESTful API for predictions.\n",
        "      * **Output:** A deployable application (e.g., Docker image) or an optimized model for edge devices.\n",
        "\n",
        "### 2.2. System Architecture for Deployment\n",
        "\n",
        "For biodiversity monitoring, ecological research, and citizen science applications, the system architecture needs to support real-time inference and user interaction.\n",
        "\n",
        "  * **Client Applications:**\n",
        "\n",
        "      * **Mobile App (Citizen Science/Field Researchers):** Users capture images on their mobile devices. The app can perform on-device inference for immediate feedback (using optimized models like TensorFlow Lite) or send images to the backend API.\n",
        "      * **Web Portal (Citizen Science/Researchers):** A web interface for uploading images, viewing predictions, managing data, and visualizing trends.\n",
        "      * **Automated Camera Systems (Ecological Research):** Devices equipped with cameras that capture images and either process them locally (edge) or send them to the cloud for analysis.\n",
        "\n",
        "  * **Backend Services (Cloud/Server):**\n",
        "\n",
        "      * **API Gateway:** Manages incoming requests from client applications, handling authentication and routing.\n",
        "      * **Prediction Service (Model Server):**\n",
        "          * **Containerization (Docker):** The trained model and its inference code are packaged into a Docker image, ensuring consistent execution across environments.\n",
        "          * **Model Serving Frameworks:** Tools like TensorFlow Serving, TorchServe, or custom Flask/FastAPI applications host the model and expose a REST API endpoint.\n",
        "          * **Scalability:** Auto-scaling groups or Kubernetes deployments can dynamically adjust resources based on demand.\n",
        "      * **Database (PostgreSQL, MySQL, SQLite):**\n",
        "          * **Species Database:** Stores detailed information about butterfly species (taxonomy, characteristics, conservation status).\n",
        "          * **Prediction Logs:** Records prediction requests, image metadata, predicted species, confidence scores, and timestamps for monitoring and auditing.\n",
        "          * **User Data:** Stores user profiles and contributed data (for citizen science).\n",
        "      * **Data Storage (Object Storage):** Cloud storage solutions (e.g., AWS S3, Google Cloud Storage) for storing raw images and processed data.\n",
        "      * **Queueing System (Optional):** For handling asynchronous prediction requests (e.g., batch processing) or processing large volumes of data (e.g., Kafka, RabbitMQ).\n",
        "      * **Monitoring & Logging:** Tools (e.g., Prometheus, Grafana, ELK Stack) to monitor application performance, model latency, and log errors or predictions.\n",
        "\n",
        "  * **Edge Deployment (Optional, for real-time field use):**\n",
        "\n",
        "      * **Optimized Models:** Models optimized for edge devices (e.g., TensorFlow Lite, ONNX Runtime, OpenVINO) reduce size and computational requirements.\n",
        "      * **Embedded Devices:** Deployment on Raspberry Pi, Jetson Nano, or custom AI accelerators for on-device inference without constant internet connectivity.\n",
        "\n",
        "## 3\\. Project Structure\n",
        "\n",
        "A well-organized project structure is crucial for maintainability, collaboration, and scalability. The \"Cookiecutter Data Science\" template provides a robust starting point:\n",
        "\n",
        "```\n",
        "butterfly_classifier/\n",
        "├── data/\n",
        "│   ├── raw/                  # Original, immutable raw data (e.g., downloaded images)\n",
        "│   ├── interim/              # Intermediate data, processed from raw (e.g., resized images, extracted features)\n",
        "│   ├── processed/            # Final, clean data ready for modeling (e.g., train/val/test splits)\n",
        "│   └── external/             # Data from third party sources (e.g., additional species data)\n",
        "├── models/                   # Trained and serialized models, model checkpoints\n",
        "├── notebooks/                # Jupyter notebooks for exploration, analysis, and prototyping\n",
        "├── references/               # Data dictionaries, APIs, research papers, project reports\n",
        "├── src/                      # Source code for the project\n",
        "│   ├── __init__.py           # Makes src a Python package\n",
        "│   ├── data/                 # Scripts for data ingestion and processing\n",
        "│   │   ├── make_dataset.py\n",
        "│   │   └── augment_data.py\n",
        "│   ├── features/             # Scripts for feature engineering (if applicable, less common for raw images)\n",
        "│   │   └── build_features.py\n",
        "│   ├── models/               # Scripts for model definition, training, and evaluation\n",
        "│   │   ├── train_model.py\n",
        "│   │   ├── predict_model.py\n",
        "│   │   └── build_model.py    # Defines CNN architecture, transfer learning setup\n",
        "│   ├── visualization/        # Scripts for creating visualizations\n",
        "│   │   └── visualize.py\n",
        "│   └── app/                  # Application code for deployment (API, web interface)\n",
        "│       ├── api.py            # Flask/FastAPI application\n",
        "│       ├── templates/        # HTML templates for web interface\n",
        "│       ├── static/           # CSS, JS, images for web interface\n",
        "│       └── database.py       # SQLite database interaction\n",
        "├── README.md                 # Project README with setup instructions, usage, etc.\n",
        "├── requirements.txt          # Python dependencies\n",
        "├── setup.py                  # For installing the project as a package\n",
        "├── Dockerfile                # For containerizing the application\n",
        "├── .gitignore                # Files/directories to ignore in Git\n",
        "├── .env                      # Environment variables (e.g., API keys, database paths)\n",
        "└── tests/                    # Unit tests for code components\n",
        "```\n",
        "\n",
        "## 4\\. Data Collection and Preparation\n",
        "\n",
        "The dataset comprises 75 classes with 6499 images. This section details how to collect and prepare such data.\n",
        "\n",
        "### 4.1. Data Collection Strategies\n",
        "\n",
        "While the dataset is given, for future expansion or similar projects, consider:\n",
        "\n",
        "  * **Diversity:** Ensure images cover various angles, lighting conditions, backgrounds, and life stages if relevant.\n",
        "  * **Real-world Representation:** Collect images that reflect how the system will be used in the field (e.g., blurry images, occlusions).\n",
        "  * **Accurate Labeling:** Manual verification or expert annotation is crucial for high-quality labels.\n",
        "  * **Class Balance:** Actively monitor class distribution during collection. For 75 classes, some will naturally have fewer samples.\n",
        "\n",
        "### 4.2. Data Preparation Steps\n",
        "\n",
        "This involves preprocessing raw images to be suitable for CNN input.\n",
        "\n",
        "**Input:** Raw image files (e.g., JPEG, PNG) stored in class-specific directories.\n",
        "**Output:** Preprocessed image tensors.\n",
        "\n",
        "#### Python Code Example (Data Loading, Resizing, Normalization):"
      ],
      "metadata": {
        "id": "sRCvqnIsI1pM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# src/data/make_dataset.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image # Pillow for image processing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "# Configuration\n",
        "IMAGE_SIZE = (224, 224) # Common input size for pre-trained CNNs\n",
        "DATA_DIR = 'data/raw/butterfly_dataset' # Path to your raw image dataset\n",
        "PROCESSED_DATA_DIR = 'data/processed'\n",
        "CLASSES = sorted(os.listdir(DATA_DIR)) # Assumes subdirectories are class names\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "\n",
        "def load_and_preprocess_image(image_path, target_size=IMAGE_SIZE):\n",
        "    \"\"\"Loads an image, resizes it, and normalizes pixel values.\"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path).convert('RGB') # Ensure 3 channels\n",
        "        img = img.resize(target_size)\n",
        "        img_array = np.array(img).astype('float32') # Convert to numpy array, float32\n",
        "        img_array = img_array / 255.0 # Normalize pixels to [0, 1]\n",
        "        return img_array\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def prepare_dataset(data_dir=DATA_DIR):\n",
        "    \"\"\"Loads all images and their labels.\"\"\"\n",
        "    all_images = []\n",
        "    all_labels = []\n",
        "    label_map = {cls_name: i for i, cls_name in enumerate(CLASSES)}\n",
        "\n",
        "    print(f\"Loading images from: {data_dir}\")\n",
        "    for class_name in CLASSES:\n",
        "        class_path = os.path.join(data_dir, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        label_id = label_map[class_name]\n",
        "        for img_name in os.listdir(class_path):\n",
        "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                img_path = os.path.join(class_path, img_name)\n",
        "                processed_img = load_and_preprocess_image(img_path)\n",
        "                if processed_img is not None:\n",
        "                    all_images.append(processed_img)\n",
        "                    all_labels.append(label_id)\n",
        "\n",
        "    print(f\"Total images loaded: {len(all_images)}\")\n",
        "    print(f\"Class distribution: {Counter(all_labels)}\")\n",
        "\n",
        "    return np.array(all_images), np.array(all_labels)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    X, y = prepare_dataset()\n",
        "\n",
        "    # Save the processed data for later use (e.g., in src/data/processed)\n",
        "    os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
        "    np.save(os.path.join(PROCESSED_DATA_DIR, 'images.npy'), X)\n",
        "    np.save(os.path.join(PROCESSED_DATA_DIR, 'labels.npy'), y)\n",
        "    print(f\"Processed data saved to {PROCESSED_DATA_DIR}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "oiB_JNQRI1pR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Input:**\n",
        "\n",
        "A directory structure like:\n",
        "\n",
        "```\n",
        "data/raw/butterfly_dataset/\n",
        "├── species_A/\n",
        "│   ├── img_001.jpg\n",
        "│   └── img_002.png\n",
        "├── species_B/\n",
        "│   ├── img_003.jpg\n",
        "│   └── ...\n",
        "└── ... (75 species directories)\n",
        "```\n",
        "\n",
        "#### **Output:**\n",
        "\n",
        "  * Prints showing progress and total images loaded.\n",
        "  * `images.npy`: A NumPy array of shape `(6499, 224, 224, 3)` containing normalized image data.\n",
        "  * `labels.npy`: A NumPy array of shape `(6499,)` containing integer labels corresponding to species.\n",
        "\n",
        "### 4.3. Data Augmentation\n",
        "\n",
        "To enhance model generalization and address the relatively small dataset size (6499 images for 75 classes, averaging \\~86 images per class), extensive data augmentation is critical.\n",
        "\n",
        "  * **Basic Augmentations:**\n",
        "      * **Geometric Transformations:** Random rotation, horizontal/vertical flipping, zooming, shifting (width/height), shearing.\n",
        "      * **Color Transformations:** Brightness, contrast, saturation adjustments.\n",
        "  * **Advanced Augmentations:**\n",
        "      * **Image Mixing:** Mixup, CutMix (combining parts of two images and their labels).\n",
        "      * **AutoAugment/RandAugment:** Learning optimal augmentation policies or applying random subsets of augmentations.\n",
        "      * **Feature Augmentation:** Applying noise or transformations in feature space.\n",
        "\n",
        "These are typically applied dynamically during training using TensorFlow's `ImageDataGenerator` or `tf.data.Dataset` API with `tf.image` operations.\n",
        "\n",
        "#### Python Code Example (Data Augmentation using Keras `ImageDataGenerator`):"
      ],
      "metadata": {
        "id": "a6OiuhtSI1pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# src/data/augment_data.py\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def get_train_val_datagen(image_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Returns ImageDataGenerators for training (with augmentation)\n",
        "    and validation (without augmentation).\n",
        "    \"\"\"\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255, # Already normalized, but good practice if loading raw\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=False, # Butterflies typically not seen upside down\n",
        "        brightness_range=[0.8, 1.2],\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    val_datagen = ImageDataGenerator(rescale=1./255) # Only normalization for validation\n",
        "\n",
        "    return train_datagen, val_datagen\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Example usage - typically this is integrated into the model training script\n",
        "    train_datagen, val_datagen = get_train_val_datagen()\n",
        "    print(\"ImageDataGenerators configured for training and validation.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "033fNoytI1pT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5\\. Dataset Splitting\n",
        "\n",
        "The dataset of 6499 images needs to be robustly partitioned into training, validation, and test sets.\n",
        "\n",
        "  * **Partitioning Methodology:**\n",
        "      * **Training Set (70-80%):** Used to train the model.\n",
        "      * **Validation Set (10-15%):** Used to tune hyperparameters and monitor performance during training to prevent overfitting.\n",
        "      * **Test Set (10-15%):** Held out completely until the very end to provide an unbiased evaluation of the final model's performance on unseen data.\n",
        "  * **Stratified Sampling:** Given 75 classes, it's crucial to use stratified sampling to ensure that each split (train, validation, test) maintains approximately the same proportion of samples for each butterfly species as the original dataset. This prevents scenarios where rare species might be entirely missing from the validation or test sets.\n",
        "\n",
        "#### Python Code Example (Stratified Data Splitting):"
      ],
      "metadata": {
        "id": "T-v4CJ2zI1pU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# src/data/make_dataset.py (continued from above, or as a separate function)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "PROCESSED_DATA_DIR = 'data/processed'\n",
        "\n",
        "def split_data(X, y, test_size=0.15, val_size=0.15, random_state=42):\n",
        "    \"\"\"\n",
        "    Splits data into training, validation, and test sets using stratified sampling.\n",
        "    \"\"\"\n",
        "    # First split: train and (temp_test + val)\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y, test_size=(test_size + val_size), stratify=y, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Second split: temp_test and val\n",
        "    # Adjust val_size relative to the temp set\n",
        "    val_rel_size = val_size / (test_size + val_size)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=test_size / (test_size + val_size), stratify=y_temp, random_state=random_state\n",
        "    )\n",
        "\n",
        "    print(f\"Train set size: {len(X_train)} ({len(X_train)/len(X)*100:.2f}%)\")\n",
        "    print(f\"Validation set size: {len(X_val)} ({len(X_val)/len(X)*100:.2f}%)\")\n",
        "    print(f\"Test set size: {len(X_test)} ({len(X_test)/len(X)*100:.2f}%)\")\n",
        "\n",
        "    # Verify stratification (optional, for sanity check)\n",
        "    print(\"\\nClass distribution in splits:\")\n",
        "    for name, labels in zip(['Train', 'Val', 'Test'], [y_train, y_val, y_test]):\n",
        "        class_counts = Counter(labels)\n",
        "        print(f\"{name}: {class_counts}\")\n",
        "\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load previously saved processed data\n",
        "    X = np.load(os.path.join(PROCESSED_DATA_DIR, 'images.npy'))\n",
        "    y = np.load(os.path.join(PROCESSED_DATA_DIR, 'labels.npy'))\n",
        "\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
        "\n",
        "    # Save split datasets\n",
        "    np.save(os.path.join(PROCESSED_DATA_DIR, 'X_train.npy'), X_train)\n",
        "    np.save(os.path.join(PROCESSED_DATA_DIR, 'X_val.npy'), X_val)\n",
        "    np.save(os.path.join(PROCESSED_DATA_DIR, 'X_test.npy'), X_test)\n",
        "    np.save(os.path.join(PROCESSED_DATA_DIR, 'y_train.npy'), y_train)\n",
        "    np.save(os.path.join(PROCESSED_DATA_DIR, 'y_val.npy'), y_val)\n",
        "    np.save(os.path.join(PROCESSED_DATA_DIR, 'y_test.npy'), y_test)\n",
        "    print(f\"Split data saved to {PROCESSED_DATA_DIR}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "k99jDhkeI1pU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Input:**\n",
        "\n",
        "  * `images.npy` (Numpy array of all preprocessed images)\n",
        "  * `labels.npy` (Numpy array of all integer labels)\n",
        "\n",
        "#### **Output:**\n",
        "\n",
        "  * Prints showing the size and percentage of each split.\n",
        "  * Prints showing class distribution within each split (for verification).\n",
        "  * Saved NumPy files: `X_train.npy`, `y_train.npy`, `X_val.npy`, `y_val.npy`, `X_test.npy`, `y_test.npy` in `data/processed/`.\n",
        "\n",
        "## 6\\. Model Building\n",
        "\n",
        "Transfer learning is the cornerstone of this project, leveraging the power of pre-trained CNNs.\n",
        "\n",
        "### 6.1. Transfer Learning Approach\n",
        "\n",
        "  * **Pre-trained CNNs:** Utilize models trained on massive datasets like ImageNet (e.g., VGG-16, ResNet50, InceptionV3, MobileNetV2, EfficientNet). These models have learned a rich hierarchy of features (edges, textures, patterns) that are highly transferable to new image classification tasks.\n",
        "      * **VGG-16:** Simple, deep architecture. Good baseline.\n",
        "      * **ResNet50:** Introduced residual connections, enabling deeper networks and mitigating vanishing gradients.\n",
        "      * **InceptionV3:** Uses inception modules for efficient computation and capturing multi-scale features.\n",
        "      * **MobileNetV2:** Lightweight and efficient, suitable for mobile and edge deployments.\n",
        "      * **EfficientNet:** Scalable family of models that efficiently scale network depth, width, and resolution. Often achieves state-of-the-art performance with fewer parameters.\n",
        "  * **Strategies:**\n",
        "      * **Feature Extraction:** Freeze the weights of the pre-trained base model and train only the newly added classification layers. This is faster and requires less data, ideal for smaller datasets or when the new task is very similar to the pre-training task.\n",
        "      * **Fine-tuning:** Unfreeze some or all of the top layers of the pre-trained base model and retrain them along with the new classification layers using a very low learning rate. This allows the model to adapt the pre-trained features to the specific nuances of the butterfly dataset, often yielding better performance for larger datasets or tasks significantly different from ImageNet. A common approach is to start with feature extraction and then fine-tune.\n",
        "\n",
        "### 6.2. Model Architecture (Example using MobileNetV2)\n",
        "\n",
        "We will use MobileNetV2 due to its balance of accuracy and computational efficiency, making it suitable for potential future mobile/edge deployment.\n",
        "\n",
        "#### Python Code Example (Model Building with Transfer Learning):"
      ],
      "metadata": {
        "id": "nL1-Bzt0I1pU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# src/models/predict_model.py\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "PROCESSED_DATA_DIR = 'data/processed'\n",
        "MODELS_DIR = 'models' # Directory where trained model is saved\n",
        "CLASSES = sorted(os.listdir('data/raw/butterfly_dataset')) # Ensure classes order matches training\n",
        "\n",
        "def evaluate_model(model_path, X_test, y_test, class_names):\n",
        "    \"\"\"\n",
        "    Loads a trained model and evaluates it on the test set.\n",
        "    Prints classification report and plots confusion matrix.\n",
        "    \"\"\"\n",
        "    print(f\"Loading model from: {model_path}\")\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "    print(\"Evaluating model on test set...\")\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(f\"Test Loss: {loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Get predictions\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"\\n--- Classification Report ---\")\n",
        "    print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(20, 18)) # Adjust size for 75 classes\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('reports/figures/confusion_matrix.png')\n",
        "    plt.show()\n",
        "    print(\"\\nConfusion matrix saved to reports/figures/confusion_matrix.png\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load test data\n",
        "    X_test = np.load(os.path.join(PROCESSED_DATA_DIR, 'X_test.npy'))\n",
        "    y_test = np.load(os.path.join(PROCESSED_DATA_DIR, 'y_test.npy'))\n",
        "\n",
        "    # Path to your trained model\n",
        "    # Assume the model is saved after training in models/ directory\n",
        "    trained_model_path = os.path.join(MODELS_DIR, 'butterfly_classifier_model.keras') # Or .h5\n",
        "\n",
        "    if os.path.exists(trained_model_path):\n",
        "        evaluate_model(trained_model_path, X_test, y_test, CLASSES)\n",
        "    else:\n",
        "        print(f\"Error: Model not found at {trained_model_path}. Please train the model first.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "0YTOVeooI1pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# src/models/build_model.py\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "\n",
        "NUM_CLASSES = 75 # Based on the project description\n",
        "IMAGE_SIZE = (224, 224)\n",
        "LEARNING_RATE = 0.0001 # A common starting point for fine-tuning\n",
        "\n",
        "def build_transfer_learning_model(num_classes=NUM_CLASSES, image_size=IMAGE_SIZE):\n",
        "    \"\"\"\n",
        "    Builds a transfer learning model using MobileNetV2 as the base.\n",
        "    \"\"\"\n",
        "    # Load the MobileNetV2 model pre-trained on ImageNet, excluding the top classification layer\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(image_size[0], image_size[1], 3))\n",
        "\n",
        "    # Freeze the base model layers (feature extraction phase)\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Add custom classification head\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x) # Reduce spatial dimensions\n",
        "    x = Dense(256, activation='relu')(x) # Additional dense layer\n",
        "    x = Dropout(0.5)(x) # Dropout for regularization\n",
        "    predictions = Dense(num_classes, activation='softmax')(x) # Output layer for 75 classes\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # Compile the model for the feature extraction phase\n",
        "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(\"Model built for feature extraction (base frozen).\")\n",
        "    model.summary()\n",
        "    return model, base_model\n",
        "\n",
        "def fine_tune_model(model, base_model, initial_epochs=10, fine_tune_epochs=20):\n",
        "    \"\"\"\n",
        "    Fine-tunes the previously built model by unfreezing some layers of the base model.\n",
        "    \"\"\"\n",
        "    # Unfreeze a portion of the base model layers for fine-tuning\n",
        "    # Typically, unfreeze later layers as they learn more specific features\n",
        "    fine_tune_at = 100 # Unfreeze layers from this index onwards (adjust based on experiments)\n",
        "    for layer in base_model.layers[fine_tune_at:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    # Re-compile the model with a very low learning rate\n",
        "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE / 10), # Reduce learning rate significantly\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(f\"\\nModel re-compiled for fine-tuning (layers from {fine_tune_at} unfrozen).\")\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, base_model = build_transfer_learning_model()\n",
        "    # In a full training script, you would then train the model\n",
        "    # history_feature_extraction = model.fit(...)\n",
        "\n",
        "    # Then fine-tune\n",
        "    # model_fine_tuned = fine_tune_model(model, base_model)\n",
        "    # history_fine_tune = model_fine_tuned.fit(...)\n",
        "\n",
        "    print(\"\\nModel building script executed. Ready for training.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "wh0lMl54I1pV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Input:**\n",
        "\n",
        "  * Pre-trained MobileNetV2 weights (downloaded automatically by Keras).\n",
        "  * `NUM_CLASSES` (75) and `IMAGE_SIZE` (224, 224).\n",
        "\n",
        "#### **Output:**\n",
        "\n",
        "  * A Keras model summary, showing the layers of MobileNetV2 and the new classification head.\n",
        "  * A compiled Keras model instance, ready for training.\n",
        "\n",
        "## 7\\. Model Testing and Performance Evaluation\n",
        "\n",
        "After training, the model's performance must be rigorously evaluated on the held-out test set to ensure its reliability and generalization capabilities.\n",
        "\n",
        "### 7.1. Key Evaluation Metrics\n",
        "\n",
        "For a multi-class classification problem like this, several metrics provide a comprehensive view:\n",
        "\n",
        "  * **Accuracy:** The proportion of correctly classified instances out of the total instances.\n",
        "      * Formula: $$\\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}$$\n",
        "      * **Interpretation:** While intuitive, accuracy can be misleading in datasets with class imbalance. A high accuracy might simply mean the model is good at classifying the majority class.\n",
        "  * **Confusion Matrix:** A table that visualizes the performance of a classification algorithm. Each row represents the instances in an actual class, while each column represents the instances in a predicted class.\n",
        "      * **Interpretation:** Helps identify specific classes that the model struggles with (e.g., frequently misclassifying one butterfly species for another).\n",
        "  * **Precision (Positive Predictive Value):** For each class, it's the ratio of correctly predicted positive observations to the total predicted positive observations.\n",
        "      * Formula: $$\\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$$\n",
        "      * **Interpretation:** High precision relates to a low false positive rate. Useful when the cost of false positives is high (e.g., misidentifying a rare species as common).\n",
        "  * **Recall (Sensitivity or True Positive Rate):** For each class, it's the ratio of correctly predicted positive observations to the all observations in actual class.\n",
        "      * Formula: $$\\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$$\n",
        "      * **Interpretation:** High recall relates to a low false negative rate. Useful when the cost of false negatives is high (e.g., failing to identify a critically endangered species).\n",
        "  * **F1-Score:** The weighted average of Precision and Recall. It tries to find the balance between precision and recall.\n",
        "      * Formula: $$2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
        "      * **Interpretation:** A good metric when you need to balance precision and recall, especially in the presence of class imbalance.\n",
        "  * **Macro-Averaged vs. Weighted-Averaged:**\n",
        "      * **Macro-averaged:** Calculates metrics independently for each class and then takes the average (treats all classes equally).\n",
        "      * **Weighted-averaged:** Calculates metrics for each class and takes the average, weighted by the number of true instances for each class (accounts for class imbalance).\n",
        "\n",
        "### 7.2. Python Code Example (Model Testing and Evaluation):"
      ],
      "metadata": {
        "id": "Ozi2cqOzI1pV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Input:**\n",
        "\n",
        "  * `X_test.npy`: NumPy array of test images.\n",
        "  * `y_test.npy`: NumPy array of true labels for test images.\n",
        "  * `butterfly_classifier_model.keras`: The trained Keras model file.\n",
        "  * `CLASSES`: A list of class names in the correct order.\n",
        "\n",
        "#### **Output:**\n",
        "\n",
        "  * Prints: Test Loss, Test Accuracy.\n",
        "  * Comprehensive Classification Report (Accuracy, Precision, Recall, F1-score for each class and overall averages).\n",
        "  * A visual Confusion Matrix plot (`confusion_matrix.png`) showing true vs. predicted labels.\n",
        "\n",
        "## 8\\. Data Production Application Building\n",
        "\n",
        "Deploying the trained model into a production application allows for real-world use in biodiversity monitoring, ecological research, and citizen science.\n",
        "\n",
        "### 8.1. Deployment Strategy\n",
        "\n",
        "The application will feature a simple web interface for image uploads and predictions, backed by a Flask API.\n",
        "\n",
        "  * **API (Flask):** A RESTful API endpoint that receives image files, preprocesses them, feeds them to the trained model for prediction, and returns the predicted species.\n",
        "  * **Web Interface (HTML/CSS/JS):** A user-friendly web page where users can upload butterfly images and see instant classification results.\n",
        "  * **Database (SQLite):** A lightweight database for logging predictions and potentially storing basic species information.\n",
        "  * **Containerization (Docker):** Packaging the entire application (Python dependencies, model, API code) into a Docker image for consistent deployment.\n",
        "\n",
        "### 8.2. Python Code Example (Flask API for Prediction):"
      ],
      "metadata": {
        "id": "p3DgpXUUI1pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# src/app/api.py\n",
        "\n",
        "from flask import Flask, request, jsonify, render_template\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "\n",
        "app = Flask(__name__, template_folder='templates', static_folder='static')\n",
        "\n",
        "# Configuration\n",
        "MODEL_PATH = 'models/butterfly_classifier_model.keras'\n",
        "IMAGE_SIZE = (224, 224)\n",
        "UPLOAD_FOLDER = 'uploads' # Temporary folder for uploaded images\n",
        "DATABASE_PATH = 'data/processed/predictions.db' # SQLite database\n",
        "\n",
        "# Load the model once when the application starts\n",
        "try:\n",
        "    model = tf.keras.models.load_model(MODEL_PATH)\n",
        "    # Ensure the model's classes are in sync with the mapping\n",
        "    # In a real project, you'd load a saved class_names list or map\n",
        "    CLASSES = sorted(os.listdir('data/raw/butterfly_dataset')) # Adjust to actual class names used during training\n",
        "    print(\"Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}. Please ensure '{MODEL_PATH}' exists and is a valid Keras model.\")\n",
        "    model = None\n",
        "\n",
        "# Ensure upload folder exists\n",
        "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "\n",
        "def preprocess_image_for_prediction(image_file, target_size=IMAGE_SIZE):\n",
        "    \"\"\"Loads an image from file, resizes, normalizes, and prepares for model input.\"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_file).convert('RGB')\n",
        "        img = img.resize(target_size)\n",
        "        img_array = np.array(img).astype('float32')\n",
        "        img_array = img_array / 255.0\n",
        "        img_array = np.expand_dims(img_array, axis=0) # Add batch dimension\n",
        "        return img_array\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing image: {e}\")\n",
        "        return None\n",
        "\n",
        "def log_prediction(image_filename, predicted_class, confidence, timestamp):\n",
        "    \"\"\"Logs prediction details to an SQLite database.\"\"\"\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = sqlite3.connect(DATABASE_PATH)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS predictions (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                image_filename TEXT NOT NULL,\n",
        "                predicted_class TEXT NOT NULL,\n",
        "                confidence REAL NOT NULL,\n",
        "                timestamp TEXT NOT NULL\n",
        "            )\n",
        "        ''')\n",
        "        cursor.execute(\"INSERT INTO predictions (image_filename, predicted_class, confidence, timestamp) VALUES (?, ?, ?, ?)\",\n",
        "                       (image_filename, predicted_class, float(confidence), timestamp))\n",
        "        conn.commit()\n",
        "    except sqlite3.Error as e:\n",
        "        print(f\"Database error: {e}\")\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    if model is None:\n",
        "        return jsonify({'error': 'Model not loaded. Check server logs.'}), 500\n",
        "\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({'error': 'No file part in the request'}), 400\n",
        "\n",
        "    file = request.files['file']\n",
        "    if file.filename == '':\n",
        "        return jsonify({'error': 'No selected file'}), 400\n",
        "\n",
        "    if file:\n",
        "        filepath = os.path.join(UPLOAD_FOLDER, file.filename)\n",
        "        file.save(filepath) # Save temporarily\n",
        "\n",
        "        input_image = preprocess_image_for_prediction(filepath)\n",
        "        if input_image is None:\n",
        "            os.remove(filepath) # Clean up\n",
        "            return jsonify({'error': 'Failed to preprocess image'}), 500\n",
        "\n",
        "        predictions = model.predict(input_image)\n",
        "        predicted_class_id = np.argmax(predictions[0])\n",
        "        confidence = np.max(predictions[0])\n",
        "\n",
        "        predicted_class_name = CLASSES[predicted_class_id] # Map ID back to name\n",
        "\n",
        "        # Log the prediction\n",
        "        timestamp = datetime.now().isoformat()\n",
        "        log_prediction(file.filename, predicted_class_name, confidence, timestamp)\n",
        "\n",
        "        # Clean up the temporary file\n",
        "        os.remove(filepath)\n",
        "\n",
        "        return jsonify({\n",
        "            'predicted_species': predicted_class_name,\n",
        "            'confidence': f\"{confidence:.4f}\",\n",
        "            'all_probabilities': predictions[0].tolist() # Optional: return all probabilities\n",
        "        })\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Initialize SQLite DB\n",
        "    conn = sqlite3.connect(DATABASE_PATH)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('''\n",
        "        CREATE TABLE IF NOT EXISTS predictions (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            image_filename TEXT NOT NULL,\n",
        "            predicted_class TEXT NOT NULL,\n",
        "            confidence REAL NOT NULL,\n",
        "            timestamp TEXT NOT NULL\n",
        "        )\n",
        "    ''')\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(f\"SQLite database initialized at {DATABASE_PATH}\")\n",
        "\n",
        "    app.run(debug=True, host='0.0.0.0', port=5000) # Run Flask app"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "one8lQ4MI1pW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Input:**\n",
        "\n",
        "  * **Web Request (POST to `/predict`):** A `multipart/form-data` request containing an image file (e.g., `butterfly.jpg`) under the field name `file`.\n",
        "  * **Model File:** `models/butterfly_classifier_model.keras`\n",
        "  * **Class Names:** `CLASSES` variable must accurately list all 75 butterfly species names in the same order as trained by the model.\n",
        "\n",
        "#### **Output:**\n",
        "\n",
        "  * **JSON Response:**\n",
        "    ```json\n",
        "    {\n",
        "      \"predicted_species\": \"Papilio machaon\",\n",
        "      \"confidence\": \"0.9876\",\n",
        "      \"all_probabilities\": [0.001, 0.0005, ..., 0.9876, ...]\n",
        "    }\n",
        "    ```\n",
        "  * **Database Entry:** A new row in `data/processed/predictions.db` logging the image filename, predicted species, confidence, and timestamp.\n",
        "\n",
        "### 8.3. HTML Code Example (Simple Web Interface):\n",
        "\n",
        "```html\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Enchanted Wings: Butterfly Classifier</title>\n",
        "    <style>\n",
        "        body {\n",
        "            font-family: Arial, sans-serif;\n",
        "            margin: 20px;\n",
        "            background-color: #f4f4f4;\n",
        "            color: #333;\n",
        "        }\n",
        "        .container {\n",
        "            max-width: 800px;\n",
        "            margin: auto;\n",
        "            background: white;\n",
        "            padding: 30px;\n",
        "            border-radius: 8px;\n",
        "            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n",
        "        }\n",
        "        h1, h2 {\n",
        "            color: #4CAF50;\n",
        "            text-align: center;\n",
        "        }\n",
        "        form {\n",
        "            display: flex;\n",
        "            flex-direction: column;\n",
        "            gap: 15px;\n",
        "            margin-top: 20px;\n",
        "        }\n",
        "        input[type=\"file\"] {\n",
        "            padding: 10px;\n",
        "            border: 1px solid #ddd;\n",
        "            border-radius: 4px;\n",
        "        }\n",
        "        button {\n",
        "            background-color: #4CAF50;\n",
        "            color: white;\n",
        "            padding: 12px 20px;\n",
        "            border: none;\n",
        "            border-radius: 4px;\n",
        "            cursor: pointer;\n",
        "            font-size: 16px;\n",
        "        }\n",
        "        button:hover {\n",
        "            background-color: #45a049;\n",
        "        }\n",
        "        #result {\n",
        "            margin-top: 30px;\n",
        "            padding: 20px;\n",
        "            border: 1px dashed #4CAF50;\n",
        "            border-radius: 8px;\n",
        "            background-color: #e8f5e9;\n",
        "        }\n",
        "        #imagePreview {\n",
        "            max-width: 100%;\n",
        "            height: auto;\n",
        "            margin-top: 15px;\n",
        "            border: 1px solid #ddd;\n",
        "            display: block;\n",
        "        }\n",
        "        .error {\n",
        "            color: red;\n",
        "            font-weight: bold;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <h1>🦋 Enchanted Wings: Butterfly Classifier 🦋</h1>\n",
        "        <p>Upload an image of a butterfly to identify its species.</p>\n",
        "\n",
        "        <form id=\"uploadForm\" enctype=\"multipart/form-data\">\n",
        "            <input type=\"file\" id=\"imageInput\" name=\"file\" accept=\"image/*\" required>\n",
        "            <button type=\"submit\">Identify Butterfly</button>\n",
        "        </form>\n",
        "\n",
        "        <img id=\"imagePreview\" src=\"#\" alt=\"Image Preview\" style=\"display: none;\">\n",
        "\n",
        "        <div id=\"result\">\n",
        "            <h2>Prediction Result:</h2>\n",
        "            <p id=\"predictionText\">Upload an image to get started.</p>\n",
        "            <p id=\"confidenceText\"></p>\n",
        "            <p id=\"errorText\" class=\"error\"></p>\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "        const uploadForm = document.getElementById('uploadForm');\n",
        "        const imageInput = document.getElementById('imageInput');\n",
        "        const imagePreview = document.getElementById('imagePreview');\n",
        "        const predictionText = document.getElementById('predictionText');\n",
        "        const confidenceText = document.getElementById('confidenceText');\n",
        "        const errorText = document.getElementById('errorText');\n",
        "\n",
        "        imageInput.addEventListener('change', function(event) {\n",
        "            const file = event.target.files[0];\n",
        "            if (file) {\n",
        "                const reader = new FileReader();\n",
        "                reader.onload = function(e) {\n",
        "                    imagePreview.src = e.target.result;\n",
        "                    imagePreview.style.display = 'block';\n",
        "                };\n",
        "                reader.readAsDataURL(file);\n",
        "            } else {\n",
        "                imagePreview.src = '#';\n",
        "                imagePreview.style.display = 'none';\n",
        "            }\n",
        "        });\n",
        "\n",
        "        uploadForm.addEventListener('submit', async function(event) {\n",
        "            event.preventDefault(); // Prevent default form submission\n",
        "\n",
        "            predictionText.textContent = \"Identifying...\";\n",
        "            confidenceText.textContent = \"\";\n",
        "            errorText.textContent = \"\";\n",
        "\n",
        "            const formData = new FormData();\n",
        "            const file = imageInput.files[0];\n",
        "            if (!file) {\n",
        "                errorText.textContent = \"Please select an image file.\";\n",
        "                predictionText.textContent = \"Upload an image to get started.\";\n",
        "                return;\n",
        "            }\n",
        "            formData.append('file', file);\n",
        "\n",
        "            try {\n",
        "                const response = await fetch('/predict', {\n",
        "                    method: 'POST',\n",
        "                    body: formData\n",
        "                });\n",
        "\n",
        "                if (!response.ok) {\n",
        "                    const errorData = await response.json();\n",
        "                    throw new Error(errorData.error || `HTTP error! status: ${response.status}`);\n",
        "                }\n",
        "\n",
        "                const data = await response.json();\n",
        "                predictionText.innerHTML = `Predicted Species: <strong>${data.predicted_species}</strong>`;\n",
        "                confidenceText.innerHTML = `Confidence: <strong>${(parseFloat(data.confidence) * 100).toFixed(2)}%</strong>`;\n",
        "\n",
        "            } catch (error) {\n",
        "                console.error('Error:', error);\n",
        "                errorText.textContent = `Error during prediction: ${error.message}`;\n",
        "                predictionText.textContent = \"Upload an image to get started.\";\n",
        "            }\n",
        "        });\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "```\n",
        "\n",
        "#### **Input:**\n",
        "\n",
        "  * **User Interaction:** A user selects an image file via the \"Choose File\" button on the web page.\n",
        "  * **Click:** User clicks the \"Identify Butterfly\" button.\n",
        "\n",
        "#### **Output:**\n",
        "\n",
        "  * **Image Preview:** The selected image is displayed on the page.\n",
        "  * **Prediction Result:** The `Prediction Result` section updates to show:\n",
        "      * \"Predicted Species: **[Species Name]**\"\n",
        "      * \"Confidence: **[Confidence Percentage]%**\"\n",
        "  * **Error Message:** If an error occurs, an error message is displayed.\n",
        "\n",
        "### 8.4. SQLite Code Example (Database for Prediction Logging):\n",
        "\n",
        "This is already integrated into the `api.py` script for simplicity, handling creation and insertion. Here's how you might query it.\n",
        "\n",
        "#### Python Code Example (Querying SQLite):"
      ],
      "metadata": {
        "id": "7Wxepa-3I1pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# src/app/database_query_example.py\n",
        "\n",
        "import sqlite3\n",
        "\n",
        "DATABASE_PATH = 'data/processed/predictions.db'\n",
        "\n",
        "def fetch_all_predictions():\n",
        "    \"\"\"Fetches all logged predictions from the database.\"\"\"\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = sqlite3.connect(DATABASE_PATH)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT * FROM predictions ORDER BY timestamp DESC\")\n",
        "        rows = cursor.fetchall()\n",
        "\n",
        "        if rows:\n",
        "            print(\"--- All Logged Predictions ---\")\n",
        "            for row in rows:\n",
        "                print(f\"ID: {row[0]}, Image: {row[1]}, Predicted: {row[2]}, Confidence: {row[3]:.4f}, Time: {row[4]}\")\n",
        "        else:\n",
        "            print(\"No predictions logged yet.\")\n",
        "\n",
        "    except sqlite3.Error as e:\n",
        "        print(f\"Database error: {e}\")\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fetch_all_predictions()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "bFwMBdXCI1pX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Input:**\n",
        "\n",
        "  * Execution of the `database_query_example.py` script.\n",
        "  * The `predictions.db` file (populated by `api.py` after predictions).\n",
        "\n",
        "#### **Output:**\n",
        "\n",
        "  * Prints a list of all logged predictions, showing ID, image filename, predicted class, confidence, and timestamp.\n",
        "  * Example:\n",
        "    ```\n",
        "    --- All Logged Predictions ---\n",
        "    ID: 1, Image: butterfly_001.jpg, Predicted: Papilio machaon, Confidence: 0.9876, Time: 2025-07-09T22:30:00.123456\n",
        "    ID: 2, Image: butterfly_002.png, Predicted: Danaus plexippus, Confidence: 0.9521, Time: 2025-07-09T22:31:15.789012\n",
        "    ```\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This project outlines a comprehensive approach to building a robust butterfly image classification system using transfer learning. From meticulous data preparation and strategic model building to thorough evaluation and practical deployment, \"Enchanted Wings\" provides a framework for real-world applications. By facilitating rapid species identification, the system can significantly contribute to biodiversity monitoring, ecological research, and engaging citizen science initiatives, fostering a deeper understanding and conservation of these remarkable insects. The provided code examples illustrate key components, offering a foundation for implementation and further development."
      ],
      "metadata": {
        "id": "znDHFzE-I1pX"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
